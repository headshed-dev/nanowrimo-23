- ```
  /etc/dhcp3/dhcpd.conf
  ddns-update-style none;
  option domain-name "hifichoice.com";
  option domain-name-servers 83.146.21.6, 212.158.248.5;
  ping-check = 1;
  default-lease-time 600;
  max-lease-time 7200;
  log-facility local7;
  subnet 192.168.95.0 netmask 255.255.255.0 {
     range 192.168.95.100 192.168.95.150;
     option routers 192.168.95.1;
     filename="pxelinux.0";
     next-server 192.168.95.20;
  ```
- {{renderer :wordcount_}}
	- I dont like reading long deffinitions of what a thing is or where it came from and having to read through a bunch of stuff to get to the point. I like to get to the point and then read the details if I need to. Often however I have, and still, find that peoples idea of 'good documentation' needs to read like an academic white paper with 'terms', 'precis', 'definitions' and 'examples' and 'references' and 'further reading' and 'links' and 'footnotes' and 'bibliography' and 'appendix' and 'glossary' and 'index' and 'table of contents' and 'acknowledgements' and 'author' and 'date' and 'version' and 'license' and 'copyright'. Buried, deap within the mighty toam is an example of use that is based, not on a concrete example, but on a hypothetical one that bares little resemblance to anything in the real world.
	- To understand PXE, you need to understand DHCP, TFTP, NFS, and PXE. DHCP is a protocol for assigning IP addresses to devices on a network. TFTP is a protocol for transferring files. NFS is a protocol for sharing files over a network. PXE is a protocol for booting a computer over a network. The PXE protocol is built on top of DHCP and TFTP. But to use PXE, having found appropriate hardware to use it, typically you just need a configuration file.
	- This might suffice
		- ```bash
		    /etc/dhcp3/dhcpd.conf
		  ddns-update-style none;
		  option domain-name "hifichoice.com";
		  option domain-name-servers 83.146.21.6, 212.158.248.5;
		  ping-check = 1;
		  default-lease-time 600;
		  max-lease-time 7200;
		  log-facility local7;
		  subnet 192.168.95.0 netmask 255.255.255.0 {
		    range 192.168.95.100 192.168.95.150;
		    option routers 192.168.95.1;
		    filename="pxelinux.0";
		    next-server 192.168.95.20;
		  ```
	- What this tells me is that the `dhcp3` daemon is running and configure in `etc`. Its `.conf` file is given a bunch of directives, each of which appear in name / value pairs and on singl lines unless, sensibly grouped with '{' and '}' braces, and that the `subnet` directive is the one that is of interest. The `subnet` directive is given a network address and a netmask. The `range` directive is given a range of IP addresses to assign to devices on the network. The `option routers` directive is given the IP address of the router on the network. The `filename` directive is given the name of the file to use to boot the device. The `next-server` directive is given the IP address of the server to use to boot the device.
	- So there we go, a whole story in a box. What my mind focusses on is `filename="pxelinux.0`. This happy little stanza tells me that the network device will be primed, when it is powered on as a part of the 'hardware boot process' of the system, to start and read this file in order to start up an operating system.
	- This might sound like gobbledeygook, it might not, depending on where you've come from or what you have interests in. To understand how technology works, you have to have an idea of how things work and this typically boils down to how a thing starts, runs and stops. This would in my early days of service management, been a key piece of a run book or support guide.
	- In early 2k, I took it on myself to build a complete operating system by downloading all of its component parts from source where ever I could, compiling each and placing them into scripts capable of orchestrating an OS. This, without completely re-inventing the wheel I achieved, as many before me have done so, using [linux from scratch](https://www.linuxfromscratch.org/). There were at the time several modes in which you could do this, where the more 'advanced' levels entailed building the OS from as few pre-built components as you could. Naturally being a sucker for punishment, I chose this path as how else would I find out how a thing really worked ?
	- Prior to this had taken a lot of learning, experience gained working with actual hardware, software, automation learning and a lot of mistakes, manual repetition, frustration, confusion, often wondering what I was doing and why I should even have the right to be there doing this thing. They call that 'imposter syndrome' these days. I call it 'being a human being' if you really are honest about where you are in the greater scheme of things. To over come you just need the right people around you, a tanacity and sense of purpose and a desire to do good.
	- My starting point for real using computers in the 'real world' was working in offices at a local company I worked for as a late teenager that made metal things. I started out as a trainee welder. Welding was something I never got the hang of but part of this training on the 'shop floor' was to operate, drive, sweat over and wrangle with every piece of heavy metal tooling and infrastructure available to a factory making hydraulically enabled tipping boxes to go on the back of heavy trucks. I mostly cut up long bits of metal, box, 'rsj', angle iron, round tuby shaped for axels and hinges using a 'band saw' and getting coverd in 'suds oil' from which I still to this day recall the smell in my nostrels as if it were yesterday. I worked on the 'press and guilly' with 'foxy', the 'press man' and for a time with 'fingers mick', who had lost most of his fingers working on the same machinery. I became good frieds with Foxy and we went to see Pink Floyd at Manchester City Football ground, the first and last time I stood on the pitch at that ground or anywhere near it.
	- After a time, my 'welding training' was brought to a close as I think the employers at that time realised that they were paying me by the hour and I punched a lot of hours. Not only had I done a lot of things on the shop floor with deadly machinery, I had started to work fork lifts, 5 tonne cranes and as I had a driving license, started on fork lifts and shunting lorries in the yard. This was all getting a bit expensive as I was in before 6 and gone after 5 most days and it was going to be cheaper to get me onto a monthy salary and get me off the shop floor, into the office where they could get me to do time cards and other admin. In time, the first 'personal computers' were finding themeslves into the work place and they needed someone to 'look after them'. I was the only one who had any idea what to do, having tinkered with as ZX Spectrum in my early teens, but that is another story.
	- The operating system of the day was then DOS, Windows was not yet a thing. Amstrad was one of the leading technologies of the day, thanks to Lord Sugar of telivisional fame here in the UK. Such a lovely chap, I'm glad to never have worked for him. Data storage was often on 5 1/4 inch floppy drives that really were 'floppy'. I think 10 inches were a thing but I never worked on anything like that and pretty quickly the 'hard case and smaller floppy drives came in that went out of use at a mighty 1.44 megabytes. I dont think I got that capacity on early systems, but memory does not serve me on that detail.
	- We initially started out with PCs that resemled 'IBM clones' of the day and which there were many and that still continues to this day. We worked with a company that came from Kiel University, local to us in the Staffordshire, Stoke on Trent area to implement a coax network that was initially based upon software. For some reason, I think mainly due to it being a bit rubbish, we went over to 'Apricot' PC's which sported then a 'hardware network', whatever that means but I suppose it to be something not dissimilar to what we have today in a network stack that is a part of the operating system. I believe the transition to Apricot from clones was something in part funded by the IT contractores that were used and at least in part driven by the hard bargains driven by my then employers. To say they 'struck a hard bargain' would be an understatement. I am still scared by the memory of dealing with them on a day to day basis, alough I've come across far more, let us say, 'challenging' people since.
	- There were only a handfull of these throughout the offices, one in accounts, one in the office far side from me used by the guy that did 'order process', the managers office next to mine where I ended up working doing a lot of data entry when he went to run his own busines on the other side of town and a main 'server' upstairs with a mighty 20 Megabyte hard drive. This broke one day and I thought we'd lost everything as there were no reliable backups at the time. One of the guys from the IT contract agency came in and he took the drive out of the pc, with the cables still connected and the outer case open. With the power connected, he held the drive in his hand and shook it in a kind of rotational wrist movement and it started to spin. I think the system was then bolted back together with everything still running and at some point a tape drive was added to do backups. I never remember a recovery being done from the tape, so who knows if it worked or not.
	- So that each computer could 'talk' to each other, the server needed to be booted and running and as far as I know, it never got turned off. I pushed tapes into it each day but aside of this, had pretty little to do with it. The network was still based on coaxial cable that formed a 'bus' that had a start and an end. Each 'end' had a 'terminator', a sort of 'cap' that screwed onto the 't' section that plugged into the back of the network card. The network cable was then passed under the floor to go to the next machine in turn and the PCs in between each had a cable in and a cable out, going to the next in the 'daisy chain'. So any breaks or 'disconnects' caused a total failure of the network. Much of my time was spent chasing connection issues untill it was realised that the 'soldered connections' we were using were failing due to Andy from the shop floor giving each cable a good tug when he thought no one was looking. I found a alternative connection piece for to join the cable to the t-piece that used a screw fit, rather than needing a solder joint. This reduced the time it took for me to isolate and fix one of the tugged cables, but it didnt stop Andy and his mates having a go.
	- This was the first instance of a 'security breach' in my IT career I guess but little did I know the horrors that I would see, live and experience in years to come. The same attitude that persisted then, is prevalent today, that of dis-belief that 'it will never happen' and 'the person didn't mean to do it', in other words, a complete lack of responsibility or willingness to confront wrong doers or bad actors until it is far, far too late.
	- In my early days, I guess I mostly thought of a PC starting up or shutting down mostly as a sort of magic trick. I had little idea outside of something called `autoexec.bat`, `msdos.sys` and `io.sys` as to there being any user input into the general operation of the boot process or life cycle. I remember at times 'editting' one or all of these at different times to get things to work when I absolutely had to. One example of this was the mighty 'soundblaster' card. Oh yes, you can get sound to work with PCs, did you know ? Well, I, working at the forefront, the very bleading edge of tech back then did indeed install sound cards into PCs of my own and even other peoples. Yes, other people tried to get me to 'fix their PC', and that happened quite a lot. Today they dont just want me to fix one PC, they expect me to fix an entire infrastructure made up of 1000s of nodes and comprising 100s of different technologies, toolsets, frameworks and other doo hickies. So the same then still applies today, that most folk dont want to or cannot for a variety of reasons, get to grips with making the technology of the day do things for them. The marjority are consigned to be more 'consumers' than 'expert users', let alone architects, developers, operators and so on. Which is a shame really. I think, like math, IT should be an integral part of anyones education, not just a 'specialist' subject for a few.
	- I suppose my experience growing up on a factory shop floor and doing lots of things, each totally different, many with the potentional for loss of limb, severe injury or even sudden death, made me 'the man that I am today'. Ha ha. But seriously folks, what I mean is, doing a 'bit of everything' is often stigmatised as becoming a 'jack of all trades and master of none'. Yet when I went through University and studied under a guy called Alan Eardley at Staffs polytechnic / university as it was when I was to graduate, we got to know about the Japanese industrial culture built around 'just in time'. This abreviates to JIT and was in opposition to the thinking of the day, being 'Manufacturing Resource Planneing', also abreviated to MRP. So the JIT MRP wars raged back in the day of my being covered in suds oil and stinking of sweat and stale cigarette smoke.
	- A core component of the JIT philosophy was the employment of a multi skilled workforce capable of moving from one area of production to the other. This is in stark contrast to the 'division of labour' methodologies of the great Ford motor plants of the day that did the counter opposite of this and led to crazy situations as I see it where humans are reduced to putting wheels onto a car in a production line and all they do, day in, day out is just that. I remember a sketch on a record ( thats a vynil record to you youtube gen Xers, you'll find them in a museum somewhere but I do believe they are having somehign of a come back ) of Peter Sellers, a great British actor, comedian and all round good egg, where he is a factory worker making tooth brushes. He is the one that 'puts the little holes in the tooth brush'. A manager comes to speak with him says he, to say a couple of words, niether of which he understood.
	- I likened in my mind the hybrid working model of JIT to be like, in nature, insect swarms work to achieve a goal. A kind of 'hive mind' enables many, small and seemingly insignificant creatures to achieve great things. We are not insects, so this effect is magnified significantly when we become hybrid in nature. When a 'bottle kneck' happenss in the production line, or any other problem that halts or affects production, the workforce 'flexes' to allocate more or less people to work in one place or another. This is a kind of 'self healing' mechanism that is built into the system. It is not a 'top down' approach where a manager has to be called to 'sort it out'. This is a kind of 'bottom up' approach where the people on the ground, the ones that know what is going on, are empowered to make decisions and take action to resolve issues.
	- Another mind set that is talked about today is 'agile', in which self actuated and responsible teams work together to achieve a goal. This is in contrast to the 'waterfall' approach where a 'project manager' is employed to 'manage' the project. This is a kind of 'top down' approach where the 'manager' is the one that makes the decisions and the 'team' are the ones that do the work. This is a kind of 'command and control' approach that is often seen in the military. It is not a 'bottom up' approach where the people on the ground, the ones that know what is going on, are empowered to make decisions and take action to resolve issues.
	- So there is 'nothing new under the sun' I guess and we have all been trying, an enphasis on trying, to achieve similar things for a very long time. I've no doubt that similar debates went on whilst building the pyramids, and even the Egyptians had slaves that were well qualified, respected and sometime privileged members of society but I would not recommend going back to their way of doing thing however it may make some quite happy for us to do so.
	- Indeed, most of the people I worked with in my early days of 'metal bashing' were each capable of 'having a go' at pretty much everything in the process of manufacturing. The only place that this did not persist was the office. Although I stopped learning to weld when I worked in the office, where I concentrated on daily paper and even time punch card admin, I still did things on the shop floor that did not involve a bic pen or carbon paper copy. When I left this place I was practiced in driving vehicles as I am licensed to up tso 7 tonnes and off road, multi leggers with 8 axels. Thinking back, it was all a bit mad. I was then and still am a hybrid, having a go at lots of things and not focussing upon one or two areas. Indeed I find myself today more of an 'entrepeneur', some may say 'entrepenerd' that co-runs a business and from one day to the next, whilst I primarily, for now, create web and mobile apps, I have to be a programmer, yes, but lots of other things, a copy writer, marketting and sales person, customer liason, accountant, lawyer, business analyst, project manager, product owner, scrum master, devops engineer, cloud architect, security expert, network engineer, systems administrator, database administrator, data scientist, data analyst, data engineer, data architect, data modeler, data visualiser, data miner, data warehouser, data lake builder, AI enginner, AI prompter and the list goes on. I can never be in this short life a master of all but I have to be a master of some. This takes time to become competent in more than one 'pillar' of knowlege but it can and is done. Indeed many people attain to levels of compentence in this way of working but few can stay in one place at a time for very long if they become pidgeon holed into one specific role or functdion.
	- regrettably, many organisations today still harp back to the halcyion days of the ford motor company, when a cooky cutter approach to doing things made so much sense and worked, at the time, for making cars. Lots of cars. They were all the same but you could have them in any colour that you wanted, so long as it was black. This is a kind of 'one size fits all' approach that is still prevalent today.
- {{renderer :wordcount_}}
	- Before PXE even became a thing, I used BOOTP (Bootstrap Protocol) in my earliest pure IT role where I managed multiple client workstations and servers in an educational environment. I worked as a lab technician for the School of Computing at the University at which I had graduated. Back then, the economy was not in good shape and work was harder to find. Coupled with not having worked as a specialist in IT but having some, basic working experience I was better placed than most but as is happening again post Covid, everyone is finding things difficult. Low pay and high responsibility has become the way people can get into work by 'buying into' it. I can't work out if this is just good for some and bad for others or if someone out there is standing to make a lot of money out of the situation. Without a fairy godmother to make it all better, I dont suppose I'll ever know or be able to do anything about it.
	- With any technology, there is often a precessor or something from which the current approach has been created. Bootp and PXE (Preboot Execution Environment) are both network protocols used for bootstrapping and provisioning network devices. Each client workstation in my 'labs' that I was assigned to supervise were started up each morning and shut down at night. Whilst some of those working at the SOC managed each system individually, I had the choice to do pretty much what I wanted just so long as the systems came up every day for student use and for tutorial sessions to commence. We had 'the summer', when students are not working, for us to each get out act together. So rather than configuring each of dozens of machines with their own network addresses, I and others used BOOTP to assign IP addresses to each machine. This required for there to be a server somewhere that could 'listen' for each system booting up, requesting an IP address for it to give one out , in contrast to a 'static' approach where each system is given a fixed IP address that does not change. I didnt care much about other settings that can also be distributed by the same technology and that has become a part of DHCP, another later technology that is a part of the network stack of most operating systems today. All I wanted was to get evertying running, so expedience was the order of the day, as it is for many in IT.
	- The history of BOOTP is that it is an older network protocol that was designed to enable diskless workstations to obtain their IP configuration information and boot files from a central server over the network and this was the mother of invention that led to its being. So as I had a bunch of PCs each with hard drives and their own peripherals, BOOTP had traditionally been used to enable older workstations to start up that each did not have a hard drive, rather, they relied upon a networked server to provide each workstation with a sort of virtual hard drive, a network bootable drive so to speak. How inneficient one may say but what do you do when you open Word or Excel now, but often as not you open it in a browser and this is executed in 'the cloud' where not just disk but compute, network and the software itself doesnt even run on your system. Everyhing is in the cloud or soon will be.
	- For it to communicate on the network, BOOTP uses UDP (User Datagram Protocol) and operates on somthing called a port, which has a number 67 (for the server) and port 68 (for the client). You can think of port being like a telephone extention in an office. So just like in the matrix when they dial for an operator.
	- BOOTP then provides basic IP address assignment, subnet mask, and other configuration details, but it does not include advanced features like dynamic IP allocation and flexible boot options as does now later DHCP protocols which do and and that can give out what is called a 'lease' that acts as a kind of time based contract that says, 'you can use this address for so long, after which you need to give it up and ask for another one.
	- BOOTP clients are identified by their hardware (MAC) address, another word to understand. A MAC address is another number that each network card has that is unique to that card. It is a bit like a serial number. The server that is listening on the other end uses this MAC address to give out a unique IP address to each client.
	- So all was good in the wood. Whilst I did not really understand any of the history, finer points of the technology, I didnt really care about any of that, just it was working. Happy days.
	- But it wasnt. I found that, as did the others, that the server, if you could call it that by todays standards, was not able to respond quickly enough to even 20 or 40 PCs or workstations starting up and asking for an IP address, all at the same time. So I thought about it and realised I had something at my disposal. Code. I had up to now learnt to write in a number of programming languages. Mostly this was at university but some of this was at work, on a placement as it was called now. I believe they call this unpaid internship now but I did get a salary, which was welcome and paid for the petrol to put in the car so I could go to do my job for a year before my final year in University. How things have changed. So ofcourse, I would write in the laguage they call C.
	- The history of the language C is in part that it was popularised by the mighty book by Kerighan and Ritchie TODO called Ansi C and everyone back then had a copy.
	- I realised I needed to get each PC to boot at a different time, so each needed to wait a varying time before it requested an IP address. I wrote a program that did just that. It was the worst code I have ever written as it was entirely based on my primitive knowlege at the time of CPU clocks, speed and timing. What I did was to write a for loop that counted up from 0 to a max, that could be varied. I think I even hard coded the number as I didnt even know how to parameterise code back then. Each PC would be given a different version of the code to have a different delay. So in effect, I had waves or cohorts of PCs that started up at different times onto to the network. When the power for each lab came on, everything started but not everthying started it's network configuration at the same time.
	- It worked. I was one of the few that did not have to walk around restarting failed PCs that had failed to get an IP address but what I hadnt accounted for was that some PCs were 'faster' than others. Clock cycles vary on PCs, even of the same specification but then, as now, nothing was consistent and some PCs had been bought at different times than others and some had been 'fixed' with donor parts from older sytstems that had died before my working there.
	- My cocky attempt at user prompts that said 'booting in X seconds....' did not count down in seconds at all. On some a second could be .5 of a real second and on other systems it could be 3 or 4.
	- Rather than getting praise or congratulations from the others for doing something clever, rather they dwelt on this oversight of my coding and made fun of the attempt, despite it actually working.
	- I learn't another lesson for life, that of 'no one likes a smart arse'.
	- When I moved on to other jobs and for other bigger and more advanced roles I continued to manage multiple systems but these rarely involved users PCs or workstations, rather, they became servers in muliples, sometimes clusters or farms. Different words and phrases often are used to describe the same thing or similar concepts but with a new brand or ideaollogy. Sometimes the same thing gets dressed up as something completely different when all that has changed is the name, but I suppose you  have to market something to make it sell. PXE does a lot that BOOTP did and is an extension of BOOTP and was developed to enhance network boot capabilities, especially for booting PCs and other devices. It builds upon BOOTP but adds additional functionality. Its more advanced features, include the ability to boot from various sources (e.g., local disk, network, CD/DVD, or USB) and enables more flexible configuration options. PXE clients are also identified by their MAC address as were BOOTP ones. PXE booting often involves using TFTP (Trivial File Transfer Protocol) to transfer boot files and images from a central server to the client machine. So another, TFTPD service is run alongside DHCPD in order to do this.
	- More services or 'daemons' as they were and are called in Unix land, need more processing power on the server in order to support them but as time went on, these got faster anyway, at least untill More's law got broken and in 2007 we saw the first multi-core CPUs where a single 'dye' had multiple CPUs stamped into its wafer. This again changes to this day where we see CPUs themselves capable of 'threads' giving the effect of another working CPU so the operating system can 'see' threads in CPUs and multiple CPUS each as logical CPUs. So it all gets a bit wierd and still software, namely the programming languages that are used to develop applications and operating systems are even now, in catch up.
	- Moore's Law is an observation and prediction made by Gordon Moore, co-founder of Intel Corporation, in 1965. He stated that the number of transistors on a microchip would double approximately every two years, which would lead to an exponential increase in computing power while reducing the cost per transistor. In essence, Moore's Law described the rapid pace of advancement in semiconductor technology and the shrinking of electronic components on integrated circuits.
	- Moore's Law held true for several decades and drove the exponential growth in computing power, making devices smaller, faster, and more affordable. However, by the mid-2000s, it became increasingly difficult to maintain the same rate of transistor scaling due to physical and technological limitations. This slowdown in transistor scaling was often referred to as the "end of Moore's Law."
	- Where PXE became a watch word amongst my colleagues and comtemporaries was in the then world of 'Sun Engineers' as we liked to call ourselves at the time. Sun Microsystems no longer exits, being absorbed and taken over by Oracle. Oracle Corporation acquired Sun Microsystems in a deal that was announced in April 2009 and completed in January 2010. This acquisition marked Oracle's expansion into hardware and systems technology, in addition to its existing software and database business. The acquisition was valued at approximately $7.4 billion and had a significant impact on the technology industry.
	- Up to the Oracle takeover, being a Sun engineer was for me a thing but by the time this happened, I had transitioned pretty much over to Linux and no longer had any leaning toward specific hardware manufacturers as was the case if you worked on large systems prior to 2005.
	- But if you were trying to be among the cool kids in Sun administration, a badge of honour was to build your own 'PXE boot server'.
	- So all of the 'daemons', BOOTPd replaced by DHCPd, TFTPd, even local DNS with Named may be used to craft a server to manage servers in not only their boot process and configuration but also in their life cycle from birth to death. By that I mean, each server or system has a life cycle in which, when they start out, the are a clean slate and dont yet serve any specific purpose. They just exist on a shared network, with little more than an address and their base operating system that typically needs to be updated and patched to the latest version as quickly as possible to avoid security breaches.
	- When up and running and updated as best can be, sofware to customise what the server or 'node' does be it a stand alone component or as a member of a cluster or 'node' needs next to be configured. All of this used to be done by hand, by a human being, but as time went on, this became more and more automated.
	- What we called a PXE boot server tried to automate at least some of this life cycle. For the most part however, each engineer would get as far as biulding the server with an operating system, applying patches and then adding its initial applications and services to serve a specific purpose. Sometimes it may be to just configure its network stack and do some updates. More advanced engineers would have done much more so as to join a cluster, add more applications, configure storage, the list could go on.
	- What became to me obvious at the time was that there was a lack of standards and frameworks that everyon could follow in order to share code, solutions and to unify the way in which servers and systems were built and managed, often leaving it to individuals to do their own thing in a sort of 'home brew' approach.
	- Others were thinking the same or similar at the time and a tool called Puppet, an automation framework, was initially created by Luke Kanies in 2005. It was written in Perl at its inception. Puppet was developed in Perl until version 0.25, which was released in 2007.
	- I had specialised in Perl as a systems programming language since my early attempts to write systems software at the university job and found C to be harder to write in where as Perl got the job done and would run on any system that had a Perl interpreter installed, enabling me to automate tasks on any system, be it a Sun, Linux, Windows or other system.
	- In 2007, Luke Kanies and other developers decided to rewrite Puppet in Ruby, and this transition to Ruby marked a significant change in the project's architecture and development. Puppet's Ruby version, which is now commonly referred to as Puppet 2.0 and beyond, became the dominant and widely adopted version of the tool.
	- I was dissapointed at this move as I thought Perl was the best thing since sliced bread but as with all things, change is something we need to embrace, not resist. I needed to be good at programming and I had up to this point done that by using a single language as much as I could. That is a good approach but as you develop as a developer you need to learn new things and sticking like glue to just one language will ultimately limit you and will innevitaby stunt your growth. I am now what you could call a polyglot programmer, using upward of 5 programming languages at a time and often needing to switch between different languages even in a single project.
	- This kind of 'context switching' is nothing new and if you can do it, you can achieve much more in less time but it takes time to reach this level of competence.
	- Many I have worked with, some I have trained, they call this 'mentoring' or it could be called 'guiding and advising', anyway, out of any group of poeple in IT some will say 'I dont program' or 'I can't program'. The latter is pretty much a no no these days and the ones that say they dont code often do code but they like to think that using 'no code' frameworks or that editing configuration files and scripts is not programming, when in point of fact it often is, particularly with tools such as Terraform, Ansible and Puppet, manifests or configurations are full on programs, just not written in a programming language that you may be familiar with.
	- Chef, another popular automation framework, was created by Adam Jacob in 2009, several years after Puppet's initial release. Chef was designed to address similar challenges in automating infrastructure and system management, but it took a different approach compared to Puppet. Chef is also written in Ruby and Erlang, and it gained popularity in the DevOps and automation communities as an alternative to Puppet.
	- So another nail in the coffin for Perl, as not only a second major framework to replace home made scripts from the likes of me but also a second major framework to replace Puppet. I say the likes of me but I switched to using Puppet as soon as I could get my head round it.
	- One job I did involved Puppet, before I had started to properly learnt it. This is common in IT and particularly Devops / SRE or whatever flavour of infra as code you work in. Things move rapidly and you need to think on your feet and start using things you perhapse havn't even heard of and become productive quickly. There may be no time to have a long learning curve and you may be expected to be productive from day one. This is not always the case but it is not uncommon.
	- So some may stil be building PXE boot servers on former Sun Microsystems servers, now with an Oracle bade but mostly the task of orchestrating builds, configuration and ongoing life cycle management of systems is no longer done by hand, rather, it is done by code and this code may or may not run on a system specifically purposed for this, as was the 'PXE boot' server of old. Orchesdtration platforms themselves enen run in the cloud and upon systems that run for barely minutes, sometimes a few seconds before each is 'destroyed' and replaced by another. This is the world of 'serverless' computing and is a world away from the days of the Sun Microsystems server and the PXE boot server.
	- My experience to date and activies driven by the terms BOOTP, PXE and 'boot server' that have become for me an obsession with code, automation and orchestration, I could summarise in a few words.
		- code, if you cant, learn to code and keep on coding
		- dont become a specialist in one thing longer than you need to be so you can learn a craft and then move on to new technologies and new ways of doing things
		- embrace change and dont resist it
	- I am still in awe of how technology has advanced from my days as a trainee welder to now but even to the point of 2005 and ther after when multiple core processors became mainstream and mobile techology began first with Apple but then with Android whch has now become the most prevalent operating system on the planet.
	- These new technologies are based on very old ones. Linux is the operating system that Android is based upon. FreeBSD is the operating system that Apple is based upon. Both of these are based upon Unix, and Sun Microsystems was a Unix company. So the world has come full circle and the Unix philosophy of 'do one thing and do it well' is still a thing.
	- The Unix philosophy is a set of cultural norms and philosophical approaches to minimalist, modular software development. It is based on the experience of leading developers of the Unix operating system. Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a "software tools" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the "Unix philosophy."
	- The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design.
- {{renderer :wordcount_}}
	-