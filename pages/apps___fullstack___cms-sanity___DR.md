- {{renderer :wordcount_}}
	- it seems that despite all attempts at best practices and good intentions, backup and restore as a part of a basic security stargegy is often overlooked by many. The increase of cloud computing has in some ways made this worse, where people think that by uplifting their business systems to the cloud that somehow, all of the security and backup issues are taken care of. This is not always the case. The same can be said of many other, previoussly seen as operational, issues such as monitoring and services management.
	- But now, what was once an in house issue, managed by a separated department under the banner of something like "IT Operations" is now a shared responsibility between the business and the cloud provider. Sometimes, there is no longer any meaningful operations function or role, having dispensed this completely to developers and the cloud providers themselves. 
	- there are still companies that speicalise in this area, but they are becoming fewer and fewer as this is a high cost activity and as the cloud will always be there and switched on as is increasingly perceived by us as the general consumers of this as a commodity, the pervepted need for this is diminishing.
	- Doing backups is something that is required for a number of reasons but primarily for disaster recovery. This is used in the description of these services related to 'business continuity' and is coined as 'DR' for short.
	- Disasters, as the word describes are things we dont want to happen and are often unexpeced. Arguably, they are also things that dont happen very often, or are unlikely to happen, when all things are considered. But, when they do happen, they can be catastrophic and can cause a business to fail.
	- One such scenario I was witness to not long ago now was where a company relied upon a database that was replicated, they thought in a resilient manner in multiple places both on and off site and in the cloud. Sounds peachy right ? You've covered all the base rules here. Backup masters would say that data doesnt exist unless it is present in 3 places, the original and 2 backups. So even without physical backups, this scenario would seem to be a solution in itself right ? 
	- that would be the case if it were not for side effects that are not immediaty obvious. Over time, this service had been developed to do many different things and to serve many different purposes, adding features and functionality along the way. Each time a new service would be added and in the world of microservies and monolithic architectures, applications are developed often alongside of each other and sometimes continue to exist in parallel to each other. Sometimes this is done so as to maintain continuity between different platforms so that the older, legacy monolithic operations and applications can be deprecated and retired over time. But what can quite often happen is that business leaders to not see fit to carry out due dilligence sufficient to deprecate fully older systems as this requires for them to implement high cost migraion projects for existing, paying customers. Rather, they will tend to leave these as is, waiting for the customers to either leave by natural atrition or for themselves to upgrade to the newer services.
	- So we have a halfway house scenario that plays out where older, monolithic applications still exist along side newer micro services. The older, monoliths are likely still configured to use the same database as the newer ones, however, they are not configured to 'fail over' to the 'cloud copies' of the database as, when they were developed, this was not a feature that was available.
	- One dark winter morning, someone did something bad. I do not know to this day what that something was but I can have an idea. The main database, the primary one, from which all other replicas were made, that ran upon a piece of hardware in a managed data center failed. Its failure was such that the operating system of that machine became unbootable. On its restart, the filesystems that it had previously mounted became corrupted beyond immediate repair. The rate of change in this database was quite high, so catching up with updates proved almost impossible. By the time the primary database could be recovered, several weeks of data were either lost or in some way questionable and in that mean time, the customers and users of the service had an informatio black out, otherwise known as a service outage.
	- Many of the users and the systems still operating that were built in the monolithic style had, if not hard coded, long forgotten configurations that pointed soley to the primary database that was no longer available. The newer, microservices were able to fail over to the cloud copies of the database and were able to continue to operate. But the older, monolithic applications were not able to do this. They were not able to fail over to the cloud copies of the database and so they were not able to operate. This meant in effect, that the business was not able to operate.