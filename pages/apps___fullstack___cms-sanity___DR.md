- {{renderer :wordcount_}}
	- it seems that despite all attempts at best practices and good intentions, backup and restore as a part of a basic security stargegy is often overlooked by many. The increase of cloud computing has in some ways made this worse, where people think that by uplifting their business systems to the cloud that somehow, all of the security and backup issues are taken care of. This is not always the case. The same can be said of many other, previoussly seen as operational, issues such as monitoring and services management.
	- But now, what was once an in house issue, managed by a separated department under the banner of something like "IT Operations" is now a shared responsibility between the business and the cloud provider. Sometimes, there is no longer any meaningful operations function or role, having dispensed this completely to developers and the cloud providers themselves.
	- there are still companies that speicalise in this area, but they are becoming fewer and fewer as this is a high cost activity and as the cloud will always be there and switched on as is increasingly perceived by us as the general consumers of this as a commodity, the pervepted need for this is diminishing.
	- Doing backups is something that is required for a number of reasons but primarily for disaster recovery. This is used in the description of these services related to 'business continuity' and is coined as 'DR' for short.
	- Disasters, as the word describes are things we dont want to happen and are often unexpeced. Arguably, they are also things that dont happen very often, or are unlikely to happen, when all things are considered. But, when they do happen, they can be catastrophic and can cause a business to fail.
	- One such scenario would be where a company relied upon a database that was replicated, it was thought in a resilient manner in multiple places both on and off site and in the cloud. Sounds peachy right ? You've covered all the base rules here. Backup masters would say that data doesnt exist unless it is present in 3 places, the original and 2 backups. So even without physical backups, this scenario would seem to be a solution in itself right ?
	- that would be the case if it were not for side effects that are not immediaty obvious. Over time, this service had been developed to do many different things and to serve many different purposes, adding features and functionality along the way. Each time a new service would be added and in the world of microservies and monolithic architectures, applications are developed often alongside of each other and sometimes continue to exist in parallel to each other. Sometimes this is done so as to maintain continuity between different platforms so that the older, legacy monolithic operations and applications can be deprecated and retired over time. But what can quite often happen is that business leaders to not see fit to carry out due dilligence sufficient to deprecate fully older systems as this requires for them to implement high cost migraion projects for existing, paying customers. Rather, they will tend to leave these as is, waiting for the customers to either leave by natural atrition or for themselves to upgrade to the newer services.
	- So we have a halfway house scenario that plays out where older, monolithic applications still exist along side newer micro services. The older, monoliths are likely still configured to use the same database as the newer ones, however, they are not configured to 'fail over' to the 'cloud copies' of the database as, when they were developed, this was not a feature that was available.
	- One dark winter morning, someone did something bad. I do not know to this day what that something was but I can have an idea. The main database, the primary one, from which all other replicas were made, that ran upon a piece of hardware in a managed data center failed. Its failure was such that the operating system of that machine became unbootable. On its restart, the filesystems that it had previously mounted became corrupted beyond immediate repair. The rate of change in this database was quite high, so catching up with updates proved almost impossible. By the time the primary database could be recovered, several weeks of data were either lost or in some way questionable and in that mean time, the customers and users of the service had an informatio black out, otherwise known as a service outage.
	- Many of the users and the systems still operating that were built in the monolithic style had, if not hard coded, long forgotten configurations that pointed soley to the primary database that was no longer available. The newer, microservices were able to fail over to the cloud copies of the database and were able to continue to operate. But the older, monolithic applications were not able to do this. They were not able to fail over to the cloud copies of the database and so they were not able to operate. This meant in effect, that the business was not able to operate.
	- It is not simple as simple as to say, back up your data and all will be ok. Complex systems are just that, complex. And when we start to rely upon software as a service, more an more, this complexity gets obfuscated from us by the service providers to which we have become wed.
	- We could claim in our minds that our buisiness or techinical operations are not as complicated as all that and we dont need to worry about complex scenarios coming back to to bit us but there remains a simple truth that we all need to answer to.
		- can we back up out data, if the answer is no, we have a problem
		- can we restore the data that we have backed up, as if we cannot, the backups we made were useless, the data is lost and we have a problem
	- I underwent a proces of due diligence therefor with Sanity CMS, as I would with any technology that I am looking at with a view to its use in production and in order for me to guage its suitabilty for customer use, enen if those customers are ignorrant of the technology that is being used to serve them.
	- What I have found in this excecise is that, on an initial, free tier of operation, sanity users are not permitted to have more than 2 projects, each of which may only have 1 dataset. A backup, taken from a dataset, may not be restored to a dataset outside of that project, so a free tier user is not able to backup and restore their data to anything other than the dataset that was used to create the backup from in the first place as they will be limited to 1 dataset per project.
	- This means that the datbase is overwritten by the data from the backup taken and this cannot be tested in a free tier environment. Testing backups by taking a backup and then restoring it in a way as to have no break in service to customers and users is generally the norm, even when using free and open source solutions. So in this case, we have to be aware that this is not a free service, really, as in order to make it operational, we need to monetise it in order to get the exta features of being able to stand up a 2nd dataset in an existing project.
	- In the case of a a total disaster however, this is still not sufficient, as if something bad happened and a person or persons with access to delete the project and its dataset were to do so, this would render all of the backups, be they from a paid or free tier, useless as the backups taken will have assets embedded that are referenced to cloud storage that is no longer available and the restoration of these will be impossible.
	- Now, this may sound a bit hypothetical but if your a small business owner and you are trying to keep costs down and to a minumum you may still find this of interest as you may be thinking that you can get away with using the free tier of this service. But logically I dont really think that is viable.
	- To use a free tier for a business may also be seen as being quite irresponsible however as if you are using a service and have paying customers you should be paying for the services that you enjoy and not expecting to get them for free. Redhat recently changed their business model to reflect this in regards to Centos being a free teir of access in effect to Enterprise Redhat Linux sources. I think that the phrase they came back with was that some people that rely on this for commercial use are themselves 'freeloaders' and that they should be paying for the services that they are using. I sort of get this but for the fact that many open source developers and community volunteers have been using this very thing for years in order that they can test and make good the software that they are developing and that they are giving away for free that Redhat are themselves benifiting from.
	- That all said, I still smell smoke when I examine both the paid and unpaid free teir with Sanity as the export / import operations still only seem to cater for backup and restore to the _same_project_ even if a separate dataset within that project is used. If total data loss were to occur, the project being lost for whatever reason, the backups taken would be useless as they would be referencing assets that no longer exist.
	- There could be a workround. One such I believe would be to somehow extract the image assets refered to in each of the backup files and to store these separately so that they can be restored prior to a distater recovery operation. Likely, the backups taken would need to be editted in some way to point to these assets that have been re-created in the new project and new dataset that is now held within. I have been able to test this up to a point by manually editting the dump files that are procuded by sanity export but so far found this to be less than adequate as even with the assets extracted to local files, further errors spewed from the import process regarding to project assets that were not found. I have asked the questions on Sanity slack and have had a response to the effect that it looks like I am trying to restore to a different project. I was not given an answer to my question to this not being possible, so I am still not sure if this is the case or not.
	- I am for the moment thinking that data restore to anything other than a project that alreay exists is practical. So if sanity export and import is your only backup and restore strategy, you may want to think again.
	- The way we build websites may differ from the way others do, particularly in regards to CMSs like Sanity, CloudCannon, Contentful and the many, many others out there. I say common as I have noted that mostly the providers themselves and developers that use these services tend to integrate tightly with their CMS and once having done so, to my way of thinking, they are then effectively locked in to that service.
	- What we tend to do is to 'code deffensively' as I like to call it. What I mean by this is that if there is a simple way to reduce coupling between our code and the services that we use, we will do so. This is not always possible but we will try to do this where we can. In the case of static site generation or 'SSG' or even for dynamic sites or applications that require back end services, there is sometimes a way to also code defensively in regards to back end as a service that we use, such as firebase and its 'firestore' database. If you go deeply into firestore database schema, there are many 'cool things' you can do where data can be nested, indexed and structured in efficent patterns. But what this immediatly means is that you have an application that is tightly coupled with the firestore database and will not likely be easy to migrate to another database or backend as a service provider. So if we can, we will use simpler but just as effective data schemas that could be migrated to other databases, if that makes practical sense to do so.
	- And in the case of SSG we would automatically archive a site that is generated from a CMS, prior to and separate to its being delployed into live service. The assets that are used to create this would also likely be decoupled from the CMS, so they would comprise of artifacts that the SSG technology is already able to consume, regardless of the CMS, such as stuctured JSON data, markdown or MDX and image assets, in the orgonal form that they would have been uploaded to a given CMS. So if the CMS went pop, we could even continue to support the static site from the assets that were backed up in our pipelines. If the CMS were to come back again, these assets could potentially be restored to a 'content lake' and normal services could resume. This is not ideal, but then it is a risk and a cost that you would need to be ready for and something that needs to be factored in.
	- As a general point I find it a little depressing at times how the details, no matter how significant in real terms, are often overlooked by business leaders and entrepeneurs alike. The devil is in the details, it is oft said but we dont like to have it brought to our attention if we can at all possibly avoid it. Bad things can happen and if not often, they can, do and will. When disasters like those we put under 'DR' happen, we know about them. For small operators, this can