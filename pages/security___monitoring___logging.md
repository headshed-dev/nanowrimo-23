- {{renderer :wordcount_}}
	- back in the day when computing was done by hippy types wearing sandals and sporting beards, everything was 'the server' on which all things ran and they talked to other servers. There were dedicated routers and things that worked as 'gateways'. I remember in particular the complexity of the mail server back then. Typically you would need to run a heavily hacked and augmented installation of sendmail in order to serve email for a university or government body that would be capable of 'talking' to all the other servers out there on the smaller connected on line world that was back then. 
	- Each server in this realm may, or may not have log files. Ideally they would so that if something were to go wrong, you could 'cat' the file to the console and read through its content to look for issues or tell tale signs of things that could become a problem if not addressed. 
	- There isn't so much of a problem with this when you only have a few 'servers' on which to do this sort of administration, however as we come back to present day where in the cloud and our own private clouds of servers in data centers, some local to us but the majority remote, there are a lot, lot more instances of not just 'servers' but things we might call services that do similar jobs to those that would have been all lumped on the one physical piece of tin. 
	- This alone makes the older methods innefective to know really what is going on and even where it is going on
	- From a security point of view, if and when a server or service gets compromised by bad actors, the first thing they will do is to remove evidence of their having done this and log files are one of the first things to be heavily changed, so quite early on, log files started to be copy / forwarded to other 'secure hosts' at least and the term 'central logging' took hold.
	- Most cloud services we use today have some kind of log service or console in which we can 'filter' and query logs and it might seem like stating the obvious but this is typically on an account or tennant basis. You go to the one place, typically to do this and this is a distillatoin of years of architectural evolution that brings us to this point. 
	- Whilst we see this sort of thing as taken for granted pretty much, and we can run quite complex queries on our logs to find things that are specific to a function, 'virtual server', database instance or what ever to find things that contain a phrase or string we are interested in, even a level of severity, these are all made possible by central logging services that store and index incoming 'events' was we may call them now but they are pretty much similar to the kind of things you'd find in a log file on a server not so long ago whilst scratching your head of long hair and shuffling your sandaled feet under the desk in yester year. 
	- Much of the activity of 'central logging' could and did take place by running a cocapheny of bash commands and scripts that people made up as they went along but even early on, tools like `syslog` that did the job on Unix systems of logging to the local filesystem somewhere likely under `/var/log/` became replaced or augmented with `syslog-ng` (ng being next gen I think) and `rsyslogd` (r being for remote I would guess) where configuring a server to use logging no longer required just switching it on and using it but now needed network configuration of where to send you logs to, once there was a central logging server set up already to receive said misifs.
	- The corps got in to the act, of course they did, and tools you could and still can buy typically for eye watering sums of money that do the same job as that of central logging but with nobs on. They too, like our friendly cloud providers, recieve, index and store logs and events and provide complex and feature rich interfaces that you can spend your career learning and becoming expert in. Once such I came across that did some of these is called Splunk. Open source folks got into the act and they produced 'https://graylog.org/' and another 'ELK' ( Elastic search, Logstash and Kabana off the top of my head ) for which the history and technology of each is ( I think at least ) fascinating. TODO :
		- Logstash - ruby inside of java as JRuby
		- Elastic - big takeover by Amazon and open source is used against us
		- Kibana - the thing a lot of poeple I think thought was doing all the work
		- Graylog - agents and like it was lazer beams in outer space
	- Each solution typically has its own mechanisms and 'dialects' you need to understand in order to query data from 'the logs' so to speak but there can be some similarites in that even today, try as many do, things from the past just wont go away, despite some people hating them and trying to make them go away, one such is `Regex` ( regular expressions ) that deserve an entire book TODO , indeed I still have somewhere my Orielly text of just that, an entire book on these things described as 'line noise' ( seemingly random characters streamed from a network or telephone line )
	-
		- 
	-  