- {{renderer :wordcount_}}
	- back in the day when computing was done by hippy types wearing sandals and sporting beards, everything was 'the server' on which all things ran and they talked to other servers. There were dedicated routers and things that worked as 'gateways'. I remember in particular the complexity of the mail server back then. Typically you would need to run a heavily hacked and augmented installation of sendmail in order to serve email for a university or government body that would be capable of 'talking' to all the other servers out there on the smaller connected on line world that was back then.
	- Each server in this realm may, or may not have log files. Ideally they would so that if something were to go wrong, you could 'cat' the file to the console and read through its content to look for issues or tell tale signs of things that could become a problem if not addressed.
	- There isn't so much of a problem with this when you only have a few 'servers' on which to do this sort of administration, however as we come back to present day where in the cloud and our own private clouds of servers in data centers, some local to us but the majority remote, there are a lot, lot more instances of not just 'servers' but things we might call services that do similar jobs to those that would have been all lumped on the one physical piece of tin.
	- This alone makes the older methods innefective to know really what is going on and even where it is going on
	- From a security point of view, if and when a server or service gets compromised by bad actors, the first thing they will do is to remove evidence of their having done this and log files are one of the first things to be heavily changed, so quite early on, log files started to be copy / forwarded to other 'secure hosts' at least and the term 'central logging' took hold.
	- Most cloud services we use today have some kind of log service or console in which we can 'filter' and query logs and it might seem like stating the obvious but this is typically on an account or tennant basis. You go to the one place, typically to do this and this is a distillatoin of years of architectural evolution that brings us to this point.
	- Whilst we see this sort of thing as taken for granted pretty much, and we can run quite complex queries on our logs to find things that are specific to a function, 'virtual server', database instance or what ever to find things that contain a phrase or string we are interested in, even a level of severity, these are all made possible by central logging services that store and index incoming 'events' was we may call them now but they are pretty much similar to the kind of things you'd find in a log file on a server not so long ago whilst scratching your head of long hair and shuffling your sandaled feet under the desk in yester year.
	- Much of the activity of 'central logging' could and did take place by running a cocapheny of bash commands and scripts that people made up as they went along but even early on, tools like `syslog` that did the job on Unix systems of logging to the local filesystem somewhere likely under `/var/log/` became replaced or augmented with `syslog-ng` (ng being next gen I think) and `rsyslogd` (r being for remote I would guess) where configuring a server to use logging no longer required just switching it on and using it but now needed network configuration of where to send you logs to, once there was a central logging server set up already to receive said misifs.
	- The corps got in to the act, of course they did, and tools you could and still can buy typically for eye watering sums of money that do the same job as that of central logging but with nobs on. They too, like our friendly cloud providers, recieve, index and store logs and events and provide complex and feature rich interfaces that you can spend your career learning and becoming expert in. Once such I came across that did some of these is called Splunk. Open source folks got into the act and they produced 'https://graylog.org/' and another 'ELK' ( Elastic search, Logstash and Kabana off the top of my head ) for which the history and technology of each is ( I think at least ) fascinating. TODO :
		- Logstash - ruby inside of java as JRuby
		- Elastic - big takeover by Amazon and open source is used against us
		- Kibana - the thing a lot of poeple I think thought was doing all the work
		- Graylog - agents and like it was lazer beams in outer space
	- Each solution typically has its own mechanisms and 'dialects' you need to understand in order to query data from 'the logs' so to speak but there can be some similarites in that even today, try as many do, things from the past just wont go away, despite some people hating them and trying to make them go away, one such is `Regex` ( regular expressions ) that deserve an entire book TODO , indeed I still have somewhere my Orielly text of just that, an entire book on these things described as 'line noise' ( seemingly random characters streamed from a network or telephone line )
	- I encountered people that claimed to be able to read the output of log files as their entries stream past on the console, quite like in 'the Matrix' but with words, not weird symbols, and could 'spot patterns' and thus percieve issues or highlight things that are 'wrong' and be able to react to them. I can admit to becoming similar to this myself but it takes a long time to get good at this. Its like speed reading x1000 I guess and is a black art that you may or may not be able to rely on. I came across someone that claimed to be so good at this, their id in slack was an eagle for 'eagle eyed' as they thought and the collective stories and myths in that place thought that nothing went past that persons gaze unnoticed.
	- This kind of 'wetware' is not good enough and we cannot rely on poeple having abilities not far from water divinatoin or would be telepethy to be our monitoring and analytics
	- Elastic and Graylog, the open source solutions I mentioned and of course the now cloud based vendor lock in ones you can find in AWS, Azure, Google et. al will all do some kind of intelligent queries on logs or events, where the frequency of things happeing may be monitored, not just the thing itself
	- This is key to managing large estates of systems as there are millions of events or logs being created at any one time and it is not enough to try to respond to every sinle one of them, rather, we need to know if a particulaar type of event is increasing in veracity or prevalance. Trends need to be plotted and predictions made, based on historical events and 'ramp ups' or contrary to that significant 'fall off' or declines be it errors, warnings, info or debug like categorisation of events. We need to be able to control the levels of logging and ideally applicaitons in our infrastructure now no longer are compiled statically, with their confifugrations 'baked in' but those that can get their configuration dynamically from a central point using 'configuration management' to allow us to turn up or down the kind of logging we want to see
	- cost is key to all this in terms of how much storage we have allocated to holding these millions or more of logs and how long we want to keep them for
	- anecdotally, I recall that at one place where someone had a bee in their bonnet about using Splunk, they got a license to use it but needed to agressively delete logs that it stored as they were so great that the costs charged for storing them was so great it broke the budget for having them. Once pruned, the logs that were left were of little value as there wasnt enough history to do meaninful analysis to find out if there were trends we needed to act upon. In another place I saw how graylog was configured to use a virtualised cluster in AWS and this worked ok for a time, not having heavy price permits on the amount of data logged however, the processing power required and the size of disk used went up and up over time untill the Graylog cluster, comprising now of 3 or more significant VMs and let us say, quite a lot of disk became one of the biggest and expensive services in that AWS tenancy
	- We talk a lot about SaaS ( Software as a Service ) and as you can imagine, tools that essentially do logging can be very, very profittable and if you can convince people that by using theirs as opposed to doing it yourself or using another solution you will gain an edge over the competition
	- What used to be the domain of the 'ops' person to implement logging and event analysis solutions has started to become the domain of the developer now as many things that used to take place in a company back office systems is now being predated by cloud providers as, after all, if we move all our servers to the cloud as is oft the case, what's the diffrecne in just using a cloud service to do something you used to ask the ops guy to set up for you?
	- One such example is a solution called 'New Relic' which I came across in my searches for central log and alert that integrates with application code and heres the rub
		- when loggin services were left for ops people to do, the devs either didnt bother to use these services or they lacked the tooling / knowlege to do this, so when operators like New Relic came onto the scene, a few lines of code and a library import into your C#, Java, Javascript etc. application later and your away to the races, were by every exception in your app now gets logged in the cloud and you can query them in an eye candy rich web console that makes it look very much you know what your doing and that your really on the case of quality control, service issues and so on
		- what went wrong with things like this from my observances were typically, again, the cost implicatons of these solutions and even when these were kept in budget and became pillars of the applicaiton monitoring and service managment, things happen in the world of business and the once contractor managed solution that did some very, very important work for medical providors came to be owned by new stakeholders that knew nothing of these arrangements and refused to pay for New Relic, having their own solutions elsewhere. Subsequently, the New Relic tenancy for this project got degraded to the 'free tier' which only permits for limited ( alhtough still generous for small to medium enterprise ) bandwidth log restrictions. Over night, the project lost the ability to read anything more than 2 days worth of logs, at the beginning of each month, rendering the logging practially useless
	- The tussle betwixt ops and dev I will leave for another misive but for now I think it suffice to say that there are problems on both sides and that just 'getting a devops person in', possibly like me will not solve a companies woes in this regard. Over the years, people in ops have been neglegent in seeing who their customers are, often as not devs, and thus failed to provide quality servivces that integrate with the dev workflows and enhanced the creation of new features that could be streamlined into automated pipelines that are tested automatically and continuoulsy. Rather, the devs, in their micro managed pressure cooker sprints have ejected versions of their new candy applications that they have created, with their own tooling and little integration with ops driven live monitoring, sometimes lacking any smoke tests, end to end experience checks or similar
		- is it any wonder then that whilst 'devops' was meant to reduce silos, disolve differences and do what people want to call 'service reliabiliy engineering' but that which I think is just another name for what we thought devops would do for us and didnt were in the mess many places are still in today ?
		- the solution ?
			- stay small enough for things to matter and choose technology wisely
			- keep the grown ups in the room when designing and planning
			- truly adopt agile principles, not just micro managed regurgitatons of waterfall, that are not agile, they're fragile
			- devops, sre, dev, ops are not
		-
		-
	-