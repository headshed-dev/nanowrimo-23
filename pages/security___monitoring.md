- {{renderer :wordcount_}}
	- Monitoring is something I could put into a category of its own but I put it under a heading of security because it is a security concern. Indeed as time has gone on, I have found it has been entirely ignored by some as we have adopted cloud based solutions and some have assumed that it is someone else's problem.
	- Monitoring is a key part of any service. It is the way we know that things are working as they should be. It is also the way we know that things are failing.
	- Both are as important. If you are in the unfortunate situation of serving customers with whom there is a bad relaitionship, that is, for whatever reason, trust is lost and whatever goes wrong, you, the supplier are seen as the first place to point the finger, then monitoring is your friend. It is the way you can prove that you are not at fault. Trying to regain trust is very difficult when it has been lost. But if you are to stand a chance of doing this, good monitoring is essential. What this does is to give you statistics that you can use to show that service has been maintained and can be used to diagnose problems, even if they are not your fault.
	- If you dont have this in place, the customer will be assured of being the ones to tell you that there is a problem. This is not a good place to be.
	- If you do have good monitoring in place, you can be the one to tell the customer that there is a problem. This is the better place to be and it will reduce the stress on your team and downtime for your customer as you will be alerted to issues before they escalate.
	- Monitoring is not just about the service itself. It is also about the infrastructure that supports the service. If you are using cloud based services, you will be relying on the cloud provider to provide the infrastructure. But you will still need to monitor the service. If you are using your own infrastructure, you will need to monitor that too.
	- A common mistake I have seen in the modern age of cloud and agile is for services and software to follow best principles in all aspects of software creation, iteration and deployment but to ignore monitoring. The overwelming mindset is from a development point of view and an over enphasis on testing being something that is done by the development team but somehow no longer needed when the software is deployed.
	- I think that this is a symptom of another trend that I have seen in the last 10 years or so. That is, the move away from the idea of a system administrator. This is a role that has been replaced by the DevOps engineer. This is a role that is more focused on the development side of things and less on the operations side. This is a role that is more focused on the software and less on the infrastructure. This is a role that is more focused on the development team and less on the customer.
	- Flippantly I call this 'DevDev' as it is a role that is more focused on the development team and less on the customer and an idea that we no longer need operational thinking.
	- it is not surprising as automation has meant that increasingly, outdated and expensive processes once done by operational staff are now done by software. This is a good thing. But it does not mean that we no longer need operational thinking. Operations popele that did not adapt to this change have been left behind. But the need for operational thinking has not gone away. It has just changed.
- {{renderer :wordcount_}}
	- Having spent a lot of my working life having used service monitoring and at the lack of solutions pre Y2k, developed bespoke solutions that integrated monitoring into many customer solutions and services, this all done by a team of specialist engineers at one of the leading IT service providers at that time, I think I am able to be a reliable witness as to its history and general adoption
	- I remember how I got involved in this was by being approached by a guy called John Connor, who worked in the supprt team for Internet Managed Services at that time. He was interested to take what he had developed with the name Webcheck to a new level. We would call this Webcheck 2.
	- A bit like Deep Thought in Hitchikers Guide to the Galaxy. The first one was pretty good but the second one was going to be bigger, better and have far more features.
	- That was the plan at any rate but the core principles of the solution never really changed. The number 1 feature was that the monitoring engine, which we called it, would do a job of checking a list of servers and services and tell us if any went out of wack.
	- This sounds simple but the things that are being monitored are hundreds of different systems and services in a farm of web services that a multitude of customers are relying on. Ther is no one size fits all that we can just point a tool at and say, 'monitor that'.
	- John, like other big minded thinkers before and since, came up with a set of principles for solving the problem and reduced each into a process and solutions that could be used by everyone in the team and based on open source software.
	- I guess the key strength of Webcheck, aside of a centralised database of all service status that could be relied upon to tell us within minutes if there was a problem, was that it had many what we called agents that could be deployed to any server and would report back to the central database but were all based on the same build and configuration that were only different in that some were for say, Solaris, some for Linux, some for Windows and so on. Each agent ran a set of standard and custom checks that were each plugins that could be written in any languages, so long as they could each be run as a command line and return a status code.
	- It was this guiding principle sthat made this solution a success I think as it meant that any devloper, operations person, whoever, that can at least write a simple script that can do some checks, print out a result and return an exit code can write a plugin that can be run by the agent and report back to the central database.
	- We today ofcourse have more monitoring solutions that we can shake a stick at but at the turn of Y2k, there were few and most were proprietary and expensive. Nagios did not exist at that time, only its predecessor, Netsaint. We used this but had already adopted Webcheck as it was there when it was needed and it worked.
	- Most software we have has been created out of need, not want and suits a specific use case, for which there was at some point not anything else there to do the job. This is how Webcheck came about. It was a solution to a problem that was not being solved by anything else at that time.
	- I got a mail From John a few years back, saying the last of the WebCheck servers had been decomissioned. It was in 2017 TODO - check date. I had left the company by 2007 so it had been running for a long time, considering its 'engine' was written by me in 2000. There were lots of moving parts in this solution and I wastnt alone in developing it. There were many people involved in its development and maintenance over the years. I was just the one that wrote the core engine and that likely got re-written by someone else at some point. I dont know. I was just glad to hear it was used for so long.
	- It has been interesting to see what we did back then and what has been done since in terms of what is monitored and what is considered a check of availability, performance and so on.
		- We had a set of basics, sort of bread an butter checks that you would do on a server.
			- Is it pingable
			- Is it responding to a web request
			- Is it responding to a database request
			- Is it responding to a mail request
			- Is it responding to a DNS request
			- Are there a list of sockets open that we are expecting
		- Other monitoring solutions I have come across since then, pretty much ended at this point and that was considered adequate for monitoring a server.
		- Often though you could end up with a service reporting back that it is 'up' when in actual fact, multiple services behind this simple check are non funcdtional. The experience for the customer is of their service being denied.
		- Next to happen is that the customers give up and go do something else, perhaps never to return or on occasion, they call up with a problem. 'My website is down'.
		- Interestingly we got quite a lot of the latter in those early days of the Internet and not all of them were nice about it.
		- One time I remember a colleague of mine taking a call from a customer that was pretty high up, a company director of the retail group I believe we were working with stating that their site was down. As we had Webcheck, my colleague was able to check if this was indeed the case for him to find that not only was the site fully funcdtional but it had been for a very long time.
		- This is another core principle of monitoring. Its not just enough to say that somehting is available right now, you need to be able to say that it has been available over time. This is what we call uptime and to have literally hundreds and thousands of service checks, each with a green light by them, each with a history of uptime, is a very powerful thing to have.
		- It turned out that the customer was not able to access the web site though and my colleague asked them to see if they could access other parts of the site, not just the front pages, then could they access another website that was completely different to the one they were trying to access. At this point it was realised that the customer could not access the internet at all and a reboot of his modem was required to get back on line.
		- This is another principle of monitoring and service provision. If you dont have evidence of your service being available, a shower of complaints from customers will be the first you know of a problem and you will take the blame for things that you are not responsible for.
		- I've experience of this first hand in several organsiations that I've worked for where monitoring has been neglected and a customer may even claim that problems they have internally are down to your service. Installing better monitoring can bring back the relationship into something more affable but you really dont want to go there in the first place. The best time to start monitoring is before you have a problem and that is from the start of the service being provided, not when things have gone wrong and reputations have been damaged.
		- We are not just happy to do the basic checks then, things we called back then 'external checks', that is checks on systems from their outside and that can all be done from a separate server, next level down we do system checks and there are basics for these that we can do on pretty much any system
			- Is the disk full
			- Is the CPU busy
			- Is the memory full
			- Is the network busy
			- Is the swap space full
			- Is the system load high
		- these can all be pretty simple checks to do, you'd think so wouldnt you ? But on large systems with many CPUs and threads to boot, where there are many filesytstems and mount points and of varying size. You can't just say '80%' full, send an alert when that filesystem is running to a petabyte as 10% of a petabyte can still be quite a way to go. But that's not good to just say, alert me when I'm 99% full, as even 1% can be quite a bit to go too. So we switch from percentage to megabytes and gigabytes and terabytes and so on. Problem solved, or is it ? What if we have a filesystem that spends much of its time nearly full but never grows, why get someone out of bed at night to find that a disk has grown by 1 megabyte in a month but breached alterting. So we need to analyise the trend of the filesystem and alert when it is growing at a rate that is not normal. At the very least we need a visual way to see this in the form of graphs and charts but ideally, a growth rate needs to be calculated and an alert sent when this is breached. For this to happen you need a database to store the figures that are captured by your monitoring systems and a way to query this database to get the information you need. This is where the next level of monitoring comes in, the database level.
		- Beyond the basics we can have a system that passes on all of these but has an application that is not working. This is where we need to go beyond the basics and start to monitor actual services that the application requires
			- Is the database responding
			- Is the web server responding
			- Is the mail server responding
			- Is the DNS server responding
			- Is the application responding
		- Depending on how good your checks are, you can still fail to pick up critical service failures unless you actually interact in some way with these services
			- can I insert a record into the database
			- can I get a record from the database
			- can I send a mail
			- can I receive a mail that I previously sent
			- can I send a web request and get a web page back and does it contdain the expected content
		- There is another core concept of managing availabity which some called 'experience check' and others call this 'end to end check' or 'customer experience check'. This is where you actually interact with the service as a customer using code to do it.
		- At the beginning of the Internet and e-Commerce web sites we had no tools that had fancy GUIs to do this so in the same way that the search engines ran 'spiders' to crawl the web, we created our own 'web clients' that did similar but not to index the internet, rather to test the availability of our own services by interacting with web sites, checking page content, follwing links, filling in forms, adding items to shopping carts and so on.
		- I have found this to be very powerful as it starts to become a test framwork that extends from the developement life cycle, so if you can develop your applications with this in mind, you can simply carry on running a subset of your integration tests as part of your monitoring solution. This is a very powerful thing to have.
		- Sadly, this is often not done and situations I have seen are where it has not only been overlooked but also made almost impossible to do, where customer service websites may only recieve requests from valid customers, with bank accounts and billing history together with a host of highly confidentail data. Runing a test in this kind of environment would require in some cases for a fake bank account to be set up, with a fake billing history and fake customer data. This is not something that is practical, even legal to do.
		- The only way really to achieve this kind of quality of service monitoring is to build it in from the start and to have a test framework that can be used for both testing and monitoring. - All too often I've been met with blank stares when trying to set up even the simplest form of interactive monitoring, on large systems and small, where testers even have asked me 'why to you need this ?'. At least we need to say 'why to _we_ need this ?' as it is not just the devops guy that needs this, it is everyone that is responsible for the service.
- {{renderer :wordcount_}}
	- a core concept of monitoring is polling. This is where you have a server that is running a monitoring agent that is polling a list of servers and services and reporting back to a central database. This is the core concept of Webcheck and Nagios and many other monitoring solutions.
	- In the early days, we had a standard of 5 minutes polling. This was considered to be a good balance between being able to detect a problem and not overloading the servers with too many requests. It worked well then and in my view it still works well today. However as monitoring solutions have become more sophisticated, we have seen the introduction of 'active' and 'passive' monitoring and polling intervals that are much shorter than 5 minutes. 2 minute is common and I have been asked to configure 60 seconds and less. Sometimes, system checks run at sub 10 seconds but I think that is is born of a lack of understanding of what monitoring is for and how it works, rather, it is inspired by the idea that in developement, a solution may be tested at micro second intervals and so why not do the same in production ? Often this is what you would tend to call load testing. But what I say is an opinion based on experience and monitoring things for a long time that are not all critical to sub 1 second. That is not to say that there are not use cases that such montioring is required. There are. But I think that these are the exception rather than the rule and if your asking a monitoring platform to work sub 1 minute or less, you need to think about what you are doing and why and Im being generous here, just polling every 2 minutes rather than 5 is going more than double your data storage, if you are to actually log all checks and have evidence of there being a service at each pollin period.
	- Data storage is something for monitoring you need to think about. Many solutions use 'time series' databases which are those that are designed specifically to hold 'rolling' data over time. The daddy of all these is RRD, Round Robin Database. I have used this in the past to great success to store time series data in particular for network monitoring where, we are chiefly concerned with network latency. A case in point was a system I worked on with Cisco engineers to manage a ( very ) wide area netowrk that had fiber optic cables layed under the sea, specifically for this customer but they would have recouped costs by selling off bandwidth, I can be sure. In order to give the service resillience, a secondary and tertiary service was also established using already existing services that went by a different route throughout Europe and was made by linking connections together. This is how most network connections work everywhere in the world and it is unusual to have single fiber connections from point to point that would almost guaruntee under 20 m/s latency. If the primary failed, the Cisco infrastrucure failed routes over to the backup services. So it was quiet expensive to set up. Little had been left over for monitoring however open source Nagios, RRD and RRDTool, a graphical front end gave us the reliability and accuracy we needed. A nice feature of RRD is that they do what they say on the tin, they 'round robin' their data, so you configure the amount of time you need to keep and as data is pushed in to an RRD database, it is 'rolled off' the other end of the conigured timescale. Further, it averages data for you in subsequent tables in the same database so that you dont lose everthing and you can see accurate short term data that you may configure to be accurate to within a minute, a second, or third or more may then be configured to hold for example hourly daily, monthly and so on.
	- Ther are other time series databases and RRD has in my view fallen out of favor. It cannot scale to the levels that are required today and it is not a 'big data' solution. It is a solution for small to medium sized data sets. But it is still a good solution for many use cases and I would not rule it out. It is still used today and I have used it in the last 5 years. Promethius for one I have found used more frequently, even when something like RRD is quite adequate but Prometheus can scale to stupid levels, so it must be better and we are all going to scale to become the size of Google, right ?
	- There is a little push back I've noticed in the mind set of architects and programmers against the idea of scaling from 1..infintiy that seems to persist today and for a while now. If you dont think your project is going to scale beyond a few users ( by which I would say only a few thousand ) why would you need something that will scale to hundreds of millions ? By doing so with everything you do, solutions are being over architected in my view. A great thinker in this regards I think is a fireship.io who talks a lot of sense on his channel about how the app your writing now is likely to fail and nobody will use it, therefore using BaaS like firebase, which has an effective free for ever tier for low volume useage is likekly to be fine for you side projecdt that nobody will ever use and you wont be force to pay $300 a month for. It makes a lot of sense. Much is said of 'vertical' .vs. 'horizontal' scaling but what is the point if you never scale to such a degree that vertical will not just do - that is, you can just buy a bigger VM if you need to and buy yourself some time before refactoring your app to scale horizontally. But hey, what do I know.
- {{renderer :wordcount_}}
	- active / passive - polling intervals, passive / polling
	- transports https, smtp, icmp
	- submarine
	- healthcheck, containers, API endpoints, 100% automated, on my laptop
	- for tomorrow, berkleydb - DNS and key stores, used now by object DBs and a thing called Deno, mongo and lots more like it
		- stupid fast
		- suited to things that match a use case, eg post code, remember that story ? certain power company using post codes for remediation, year or so later they end up on watchdog for sending bailiffs to propoerties of which were not even customers. But they we able to use postcodes efficienttly were'nt they ? I wonder how they did that. I have an idea. give someene a hign poweredd car, they can still drive it into a wall
		- another key value db, cockroachdb - check it out TODO with a layer of query engine atop with Postgresql interface, but under the hood, a key value db non the less
		- hash dbs, fundamental of computer science.
		- other db related for monitoring, sql we used 3.x TODO and now at TODO, now mariadb - how did it get there ? but was how we stored producxtion data and time series data
		- other dbs that are used for such and not intended for that, Elastic TODO, and a silly conversation I had with a russian developer saying I was wrong saying you need to use time series for time based data and Elastic ( ELK ) is the way. interesting tech blindness and in the face of failed use of nerd life
		- another TODO, wasnt there an extention to another time series DB, was it promethius? or similar, to  manage data that is document based, like Elastice, that is then another 'cross over'.
		- Casandra, with and SQL like interface, capabable of 'horizontal scaling' yada yada
		- SQLite - it can work in cluster mode, yes it can and can handle petabytes of data, so deal with it
		- Postgresql has a lot of featrures that Oracle has and is open source, go figure
		- the filesystem, believe it or not is pretty damn quick - a conversation with the author of Exim, Philip Hazel at a dinner in Oxford with Dave Markham, he was an amateur dramatic enthusiast but his test rig was legenendary
			- oh, and a keynote speaker from Fastnet, that was in love with C as a programming language, yep, since 1995 their the ones
		- we glibbly say 'use a database' but what does that mean ? At night school, over 30 years ago now, I to college in day release from my day job where I worked for an engineering company and worked in their offices maintaining a small network of Apricot PCs. Hungry for knowlege and wanting to 'program the network', a phrase I recall that I used to define my objectives at the time, I started formally learning computer science principles. Amongst many other concepts I needed to ingest was the idea of storing data on disk and in memory. The CPU doesnt really care which you use but however you do it, you need some kind of strategy for writing and then retrieving data. Ideally, data is written to disk in order that it may be persisted between reboots or power loss. The latter is less of a problem that you would think practically as modern systems can maintain power through power cuts, brown outs and hardware is highly engineered now ( in enterprise level data centers ) to maintain memory state. The simplest case when persisting data is to write a file. It has a name, its filename and this is used to write or read from the file, ingesting its content into a buffer in memory so we can use this data. As our data storage requirements need to be structured, in the case of a database, there needs to be a strategy for storing this to disk as these files can become quite big and reading the whole file into memory may take too long or simply exceed the amount of memory we have available. So at the lowest level, the database file needs an index, sort of like the contents page in a book, that can be used to find where blocks of data is located on disk. With a spinning hard drive this would have literally equated to locations to which the read arm could be moved to in order to locate a given sector of data, also called doing a 'seek'. We aren't really aware of this low level activity when programming unless working at a low end, at the system level and I guess if you are, your programming a database or similar, working on a kernel driver or similar. What this means is that data may be accessed very quicky by looking up its location, using a 'key' and then, the contents we need, the 'value' is available quickly and efficiently. This is an over simplification of course as data grows in size and complexity, we end up with indexes that have indexes in tree like structures and we employ patterns or algorythms to efficiently search for and retrieve data, using multiple indexes and trees of indexes. This is what a database does for us. It is a way of storing data on disk and retrieving it efficiently.
		- This method of storing and retrieving structured data is typically implmented in a way that we can more easily deal with in code by key-value databases. A key-value database is a type of nonrelational database that stores data as a collection of key-value pairs, where a key is a unique identifier for a value. The value can be any type of data, such as a string, a number, or a complex object. Key-value databases are often used for fast and scalable data access, as they do not require complex queries or schemas.
		- Berkeley DB is an example of a key-value database. It is an embedded database software library that provides high-performance data management services to applications. Berkeley DB supports multiple data items for a single key, transactions, concurrency control, and replication. Berkeley DB is written in C and has bindings for many other programming languages.
		  ```
		  Source: Conversation with Bing, 09/11/2023
		  (1) Berkeley DB - Wikipedia. https://en.wikipedia.org/wiki/Berkeley_DB.
		  (2) Oracle Berkeley DB. https://www.oracle.com/database/technologies/related/berkeleydb.html.
		  (3) What is Berkeley DB? - Oracle. https://docs.oracle.com/database/bdb181/html/programmer_reference/intro_dbis.html.
		  (4) What Is a Key-Value Database?. https://aws.amazon.com/nosql/key-value/.
		  (5) What Is A Key-Value Database? | MongoDB. https://www.mongodb.com/databases/key-value-database.
		  (6) Key–value database - Wikipedia. https://en.wikipedia.org/wiki/Key%E2%80%93value_database.
		  (7) What is a Key Value Database? Definition & FAQs | ScyllaDB. https://www.scylladb.com/glossary/key-value-database/.
		  ```
		- This results in a scenario where we can in a few lines of code, create, using an application program interface (API) a 'database connection' to a file on disk and then we can read records by giving the API a key or write a new value by suppling both a key and its value. This is simple do do now and, bliteringly fast on modern hardware. We are not limited greatly by the amount of data that can be stored as the underlying database will manage for us and efficient way of storing and retrieving data, despite the size of the database file on disk. Limitations are likely to be more, the filesystem or physical hardware 'hard limits', such as the amount of disk space available or the amount of memory available to the process that is running the database. This is a very simple way of storing and retrieving data and is very fast. It is also very simple to implement and use.
			- Examples of this being very effective that I can bring to mind are where I have needed to use
		- So when appropriate, I will use key-value database such as BerkeyDB whenever I can as this is cheap, fast and efficient.
		- Relational databases are a different beast. They are a way of storing data in a structured way that is more complex than a key-value database. They are also more complex to use and implement. They are also slower and more expensive to run. So why would you use one ? Well, they are more flexible and can be used to store data that is more complex than a key-value database. They are also more flexible in that they can be used to store data that is not structured in a way that is known in advance. This is a key feature of a relational database. It is a way of storing data that is not known in advance. This is a very powerful feature and is the reason why relational databases are used so widely.
		- Where as a relational database stores data in tables with predefined columns and rows, and uses SQL (Structured Query Language) to query and manipulate the data, relational database also supports ACID (Atomicity, Consistency, Isolation, Durability) guarantees, which ensure the reliability and integrity of the data. Examples of relational databases are MySQL, PostgreSQL, and Oracle a NoSQL database stores data in a different format, such as documents, key-value pairs, graphs, or columns. A NoSQL database does not use SQL, but rather its own query language or API to access and modify the data. A NoSQL database typically does not provide ACID guarantees, but rather focuses on scalability, performance, and availability. Examples of NoSQL databases are MongoDB, Redis, Neo4j, and Cassandra.
		  ```
		  Source: Conversation with Bing, 09/11/2023
		  (1) Relational Vs. Non-Relational Databases | MongoDB | MongoDB. https://www.mongodb.com/compare/relational-vs-non-relational-databases.
		  (2) Relational vs. NoSQL data - .NET | Microsoft Learn. https://learn.microsoft.com/en-us/dotnet/architecture/cloud-native/relational-vs-nosql-data.
		  (3) NoSQL vs. relational: Which database should you use for your app?. https://devblogs.microsoft.com/cosmosdb/nosql-vs-relational-which-database-should-you-use-for-your-app/.
		  ```
		- In cloud computing, implementing a database is almost entirely abstracted to a few mouse clicks or a short stanza of infrastructure as code, be it Terraform, ARM, AWS Cloud Formation, etc. but understanding what is going on 'under the hood' so to speak matters, as the underlying principles of one solution or another will still apply behind that abstraction. If you choose the wrong technology fo your app, it may still work but you may have a nasty surprise when you get the bill at the end of the month.
- {{renderer :wordcount_}}
	- So here in is a word of warning warning for cloud computing in particular. Be careful when choosing which database to use and why. If you are not sure, ask someone who does know or use an agency or contractor that does know. I have often seen situations where the wrong databas solution has been chosen for just simple, proof of concept projects or for development environments that have been configured to use enterprise level databases that are not required.
	- Choosing the right database is like choosing the right tool for the job and where I've seen this to be particularly effective is where for instance I've been able to ingest large datasets like here in the UK where we have a 'post code' database comprising of a code and a value, not discimilar to that of 'zip codes' elsewhere. This data in text form is inpractical to use when looking up more than a handfull of records, however when imported into a key-value database, it can be looked up in a fraction of a second. A relaitional databaase or a document database would be overkill for this use case but if we needed somehting more flexible, for example a database of customer orders for instance in an e-Commerce solution or a CRM of some kind, then a relational database or a document database would be more appropriate. Depending on the skills set of the development team, a relational database or one having an SQL based query interface might be favoured over one that is not but I have encountered dev teams that dont like SQL and prefer document based soltions. Users of ORM ( Object Relational Mapping ) tools may also prefer to use a relational database as these tools are designed to work with them. Some document based soluitons such as Cassandra have an SQL like interface which I've seen as a reason to favour this database over others, additional to its high avaiability and capability to scale horizontally as was as verttically. An SQLite database I have seen to be criticised due to its simple architecutre, using a single file, similar to things like BerkleyDB of old. This database however is capable of addressing vast amounts of data extremely efficiently and not requiring anything like the resources consumed by a more traditional database such as Oracle, Postgres, MySQL, MSSql and so on. Subsequently, SQLite is frequently used in edge computing, embeded applications and mobile apps in particular due to its low overheads, availability on many different platforms and low opering costs. Yet it can also be engineered to run in clustered configurations where by data in each of its nodes is replicated for high availability and not a lot of poeple know that, or seemingly so, perhaps then jumping to a wrongful conclusion that SQLite is not fit for the enterprise. Historically, Redis, a key value database that has been around for years now was also percieved as something only fit for 'caching' data however it, over time has been developed to have more features, one which is data persistence, meaning that if configured correctly, it can persist data almost as reliably as its counterparts such as Postgres et. al yet still delvier its legendary read access speed, serving data from an in memory database. So there are many factors to consider when choosing a database solution.
	- Not only choosing a database but then maintaing it else architecting its configuration to be fit for production is something else to consider and running up any database in the cloud may be a few clicks away but this still applies here also. I have encountered poorly configured databases running on MySQL for instance. I used this to persist data for Webcheck back in the day when it was at only version 3. I think we are at 8 something now and MariaDB, a fork of MySQL is at 11. We needed to store gigabytes of data back then but I've seen this scale to terrabytes and approach petabytes in production environments since. Failing to configure an index on a large table of data I once saw resulting in over 1 million row scans per query. I found this by recording current queries on a web server, plotting each against the database using its 'explain' feature to detect queries that are expensive. There were 1000s of queries a minute to deal with and the task was non trivial, however the addition of a single index on a single column reduced the number of row scans to less than 100. The server was 'falling to its knees' prior to this and after, it was running at a fraction of its capacity. This is a simple example of how a database can be configured to be more efficient. There are many more examples of this and I have seen many databases that have been configured poorly. This is not just a cloud thing, it is a thing that has been going on for years and will continue to do so.
	- I am critical of a trend that is present in systems architecture and governing practice where operational, service related management is forgotten where developer roles have taken over operational ones however, when Google published their internaly named system they call 'Borg'.
	- Google introduced Kubernetes as an open source version of Borg in mid-2014¹². Borg was a large-scale internal cluster management system that ran hundreds of thousands of jobs at Google since the early 2000s¹³. Kubernetes was inspired by Borg, but not directly derived from it¹.
	  ```
	  Source: Conversation with Bing, 09/11/2023
	  (1) From Google to the world: The Kubernetes origin story - Google Cloud. https://cloud.google.com/blog/products/containers-kubernetes/from-google-to-the-world-the-kubernetes-origin-story.
	  (2) The History of Kubernetes on a Timeline - RisingStack Engineering. https://blog.risingstack.com/the-history-of-kubernetes/.
	  (3) The history of Kubernetes - IBM Blog. https://www.ibm.com/blog/kubernetes-history/.
	  ```
	- In a kubernetes service, healthchecks are configured and these healthchecks can be as simple or as complicated as you want them to be but out of the box, they do a pretty good job of monitoring availability. I have to take my hat off to them, as this 'feels' very similar to the service management I was accustomed to from pre Y2K and in the days of developing Webcheck with John Connor and colleagues.
	- Google implemented solutions in their own infrastructure well before ever publishing this to the outside world, yet built in to the very orchestration platform itself, the ability to monitor service availabily, not just by simple external checks but by internal checks that can be configured to be as simple or as complicated as you want them to be. When this was open sourced, it has been adopted by many organisations and is now the de facto standard for container orchestration.
	- Despite this I have still encountered what has amounted to resistence in some quarters to simply a lack of awareness of this feature in others and even senior developers I have helped understand the need for 'health checks' as they are called in K8s ( kubernetes ) and other container orchestration platforms.
- {{renderer :wordcount_}}
	- databases are often being updated to include new features, so earlier, when we said that relaitional databases differ from document based databases, this is not strictly true as some relaitional databases, such as MySQL have the capability of handling document based data. This is a good example of how databases are evolving and how they are converging in some cases.
	- Kubernetes will innevitably change the way we manage and deploy applications.
	- Before anyone started to adopt it, even in small companies, cloud providors offering Platform as a service, such as Azure for instance, often will be using kubernetes or similar in their back-ends and when implementing applications in their platforms as docker containsers or web applications, we will occaisionally get a peek at the underlying infrastrucure when viewing deploy logs. Other container orchestration platforms are available, such as Docker Swarm, Mesos and others but Kubernetes is becoming the de facto standard.
	- when choosing one kind of database over another, I often think back to an evening I had the honour of spening with a guy called Philip Hazel, at Cambridge University once. I was on a course for Exim ( Experimental Mail Server ) as it was this tehat we ran alongside Webcheck. I was in atendance with a Dave Markham, a fellow colleague in that team and we were each allocated a table at dinnder that evening. We happened to be placed with Philip. Whilst we avoided converstation regarding Exim as that was the subject of the days discuourse, we inevitably came back to Exim and databases vs the filesystem. Phillip Hazel said somethitng that struck me then as now, he said that in some use cases, the filesystem, believe it or not is pretty quick so it is not always necessary to use a database. I often recall this insight when architecting anything and consider if it is necessary to implement a component such as a database, cache, web server, message queue, asking myself, are we over engineering here, sometimes, keeping is simple is the best way to go.
- {{renderer :wordcount_}}
	- here